{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "For this example, we using PyPI package: https://pypi.org/project/ollama/\n",
    "\n",
    "Here you can find official documentation for `Ollama` Python library: https://github.com/ollama/ollama-python\n",
    "\n",
    "Here is official documentation how to install `Ollama`: https://ollama.com/download"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's start from simple things. What models are available to us?\n",
    "\n",
    "It's very easy to do using `Ollama`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import ollama\n",
    "\n",
    "# This will return a list of all downloaded previous models.\n",
    "print(ollama.list())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's make it adequately formatted and easy to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for model in ollama.list()[\"models\"]:\n",
    "    print(f\"- '{model['name']}' of family '{model['details']['family']}', in format '{model['details']['format']}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Let's pick some simple model for our examples. I think that small model of `qwen2` family should work fine for simple text generatin examples.\n",
    "\n",
    "- [qwen2](https://ollama.com/library/qwen2): `qwen2:0.5b`, `qwen2:1.5b`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "USE_MODEL = \"qwen2:0.5b\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If this model is on the list above, then there are no problems. We should be able to use it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try to generate answer for a simple question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.generate(USE_MODEL, 'Why is the sky blue?')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "But sadly, we don't have that model downloaded, and we need to download it first to start work with that model.\n",
    "\n",
    "To do that, the easiest way is to use `ollama.pull`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ollama.pull(USE_MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note that here we using small model for simplicity and generation speed.\n",
    "\n",
    "To get more reasonable and consistent answers, consider using `mistral:7b` or `llama3:8b`.\n",
    "\n",
    "And now we should be able to get our answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = ollama.generate(USE_MODEL, 'Why is the sky blue?')\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Exactly the same example as above but with streaming ability; in this case, LLM will generate an answer as a typewriter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stream = ollama.generate(USE_MODEL, 'Why is the sky blue?', stream=True)\n",
    "for chunk in stream:\n",
    "  # by default `end` is set to `'\\n'` and `flush` is not set.\n",
    "  # print(\"...\", end='\\n')\n",
    "  print(chunk[\"response\"], end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "Note that `ollama.generate` is, in reality, `client.generate` and `client` is an instance of `ollama.Client` with no parameters.\n",
    "\n",
    "That's how creating all these functions looks in the `ollama` module.\n",
    "\n",
    "```python\n",
    "\n",
    "_client = Client()\n",
    "\n",
    "generate = _client.generate\n",
    "chat = _client.chat\n",
    "embeddings = _client.embeddings\n",
    "pull = _client.pull\n",
    "push = _client.push\n",
    "create = _client.create\n",
    "delete = _client.delete\n",
    "list = _client.list\n",
    "copy = _client.copy\n",
    "show = _client.show\n",
    "ps = _client.ps\n",
    "```\n",
    "\n",
    "Then, by using `ollama.list`, `ollama.pull` or `ollama.generate` or any other method, you use the methods of the `Client` class instance.\n",
    "\n",
    "However, if the `ollama` or `ollama-docker` container is not installed on the current machine, there can be a problem.\n",
    "\n",
    "It's impossible to set a `host` parameter to define a host with which to interact."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "To solve the problem with the configuration of the host, we can use the `Client` class directly to create a client instance and set the `host` parameter.\n",
    "\n",
    "If the `Ollama` server is installed on the current machine, just use `http://localhost:11434`; for the remote machine, you will need to point to the exact machine address/IP and port. example `http://192.168.1.100:11434`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ollama.Client(host='http://localhost:11434') # In this example, we have `Ollama` installed locally.\n",
    "\n",
    "# Here we using `ollama.generate` but with our own instance of `ollama.Client`\n",
    "response = client.generate(USE_MODEL, 'Why is the sky blue?')\n",
    "\n",
    "print(response['response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same example again uses streaming instead of waiting until the whole answer is generated.\n",
    "\n",
    "Note that the structure of `chunk` has changed, and we now need to use these keys to access the current message part `chunk['message']['content']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = ollama.Client(host='http://localhost:11434')\n",
    "\n",
    "stream = client.chat(\n",
    "    model='qwen2:0.5b',\n",
    "    messages=[{'role': 'user', 'content': 'Why is the sky blue?'}],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in stream:\n",
    "  print(chunk['message']['content'], end='', flush=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
